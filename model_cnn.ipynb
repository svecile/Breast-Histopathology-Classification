{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import wandb\n",
    "from ece9603_project import lossFunctions\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mece9603_project\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run once to setup connection to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xl3j0om6\n",
      "Sweep URL: https://wandb.ai/ece9603_project/breast-histopathology-classification/sweeps/xl3j0om6\n"
     ]
    }
   ],
   "source": [
    "# Setup sweep hyperparameters\n",
    "sweep_config = {\n",
    "    'name': 'loss_function_evaluation',\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'loss_function': {\n",
    "            'values': ['dice_loss',\n",
    "                       'bce_dice_loss',\n",
    "                       'jaccard_iou_loss',\n",
    "                       'focal_loss',\n",
    "                       'tversky_loss',\n",
    "                       'focal_tversky_loss',\n",
    "                       'bce_with_logits_loss']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       project=\"breast-histopathology-classification\",\n",
    "                       entity=\"ece9603_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.info = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.info.imgPath.values[idx]\n",
    "        label = self.info['class'].values[idx]\n",
    "        image = read_image(path, mode=torchvision.io.image.ImageReadMode.RGB).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient  class  posX  posY                                         imgPath\n",
      "0    12954      0  1151  1401  data/12954/0/12954_idx5_x1151_y1401_class0.png\n",
      "1    12954      0  1951  2901  data/12954/0/12954_idx5_x1951_y2901_class0.png\n",
      "2    12954      0   151   501    data/12954/0/12954_idx5_x151_y501_class0.png\n",
      "3    12954      0  1701  2251  data/12954/0/12954_idx5_x1701_y2251_class0.png\n",
      "4    12954      0  1501  2001  data/12954/0/12954_idx5_x1501_y2001_class0.png\n",
      "Number of Unique Patients:  279\n",
      "   patient  class  posX  posY                                         imgPath\n",
      "0    12954      0  1151  1401  data/12954/0/12954_idx5_x1151_y1401_class0.png\n",
      "1    12954      0  1951  2901  data/12954/0/12954_idx5_x1951_y2901_class0.png\n",
      "2    12954      0   151   501    data/12954/0/12954_idx5_x151_y501_class0.png\n",
      "3    12954      0  1701  2251  data/12954/0/12954_idx5_x1701_y2251_class0.png\n",
      "4    12954      0  1501  2001  data/12954/0/12954_idx5_x1501_y2001_class0.png\n",
      "Number of Train Patients:  195\n",
      "      patient  class  posX  posY  \\\n",
      "6566    15634      0  2401  1801   \n",
      "6567    15634      0  1001  1651   \n",
      "6568    15634      0  2051  1351   \n",
      "6569    15634      0  1301  2201   \n",
      "6570    15634      0  1351  1301   \n",
      "\n",
      "                                             imgPath  \n",
      "6566  data/15634/0/15634_idx5_x2401_y1801_class0.png  \n",
      "6567  data/15634/0/15634_idx5_x1001_y1651_class0.png  \n",
      "6568  data/15634/0/15634_idx5_x2051_y1351_class0.png  \n",
      "6569  data/15634/0/15634_idx5_x1301_y2201_class0.png  \n",
      "6570  data/15634/0/15634_idx5_x1351_y1301_class0.png  \n",
      "Number of Validation Patients:  42\n",
      "      patient  class  posX  posY  \\\n",
      "7926    13687      0  2251   251   \n",
      "7927    13687      0  2801   751   \n",
      "7928    13687      0  1551   701   \n",
      "7929    13687      0  1251   651   \n",
      "7930    13687      0  2851  1251   \n",
      "\n",
      "                                             imgPath  \n",
      "7926   data/13687/0/13687_idx5_x2251_y251_class0.png  \n",
      "7927   data/13687/0/13687_idx5_x2801_y751_class0.png  \n",
      "7928   data/13687/0/13687_idx5_x1551_y701_class0.png  \n",
      "7929   data/13687/0/13687_idx5_x1251_y651_class0.png  \n",
      "7930  data/13687/0/13687_idx5_x2851_y1251_class0.png  \n",
      "Number of Test Patients:  42\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"breastCancerDataframe.csv\", index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "patientIDs = df.patient.unique()\n",
    "print(\"Number of Unique Patients: \", len(patientIDs))\n",
    "\n",
    "patients_train, temp = train_test_split(patientIDs, test_size=0.3, random_state=42)\n",
    "patients_val, patients_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train = df.loc[df['patient'].isin(patients_train)]\n",
    "print(df_train.head())\n",
    "print(\"Number of Train Patients: \", df_train.patient.nunique())\n",
    "\n",
    "df_val = df.loc[df['patient'].isin(patients_val)]\n",
    "print(df_val.head())\n",
    "print(\"Number of Validation Patients: \", df_val.patient.nunique())\n",
    "\n",
    "df_test = df.loc[df['patient'].isin(patients_test)]\n",
    "print(df_test.head())\n",
    "print(\"Number of Test Patients: \", df_test.patient.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomImageDataset(df_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(df_val, transform=transform)\n",
    "test_dataset = CustomImageDataset(df_test, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Percent Neg: \", (df_train['class'].to_list()).count(0)/len(df_train))\n",
    "print(\"Percent Pos: \", (df_train['class'].to_list()).count(1)/len(df_train))\n",
    "\n",
    "df_train_neg = df_train.loc[df_train['class']==0]\n",
    "df_train_pos = df_train.loc[df_train['class']==1]\n",
    "print(\"Neg Count: \", len(df_train_neg), \"Pos Count: \",len(df_train_pos))\n",
    "\n",
    "#split it into 3 since i would rather the model have an over represented positive class so the model\n",
    "#predicts pos more than neg since identifying pos is more important\n",
    "df_train1, temp = train_test_split(df_train_neg, train_size=1/3, random_state=42)\n",
    "df_train2, df_train3 = train_test_split(temp, train_size=1/2, random_state=42)\n",
    "df_train1 = pd.concat([df_train1, df_train_pos], ignore_index=True)\n",
    "df_train2 = pd.concat([df_train2, df_train_pos], ignore_index=True)\n",
    "df_train3 = pd.concat([df_train3, df_train_pos], ignore_index=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(25,5))\n",
    "sns.countplot(df_train['class'], ax=ax[0], palette=\"Reds\")\n",
    "ax[0].set_title(\"Original Train Data\")\n",
    "sns.countplot(df_train1['class'], ax=ax[1], palette=\"Blues\")\n",
    "ax[1].set_title(\"Balanced Train Data 1\")\n",
    "sns.countplot(df_train2['class'], ax=ax[2], palette=\"Greens\")\n",
    "ax[2].set_title(\"Balanced Train Data 2\")\n",
    "sns.countplot(df_train3['class'], ax=ax[3], palette=\"Oranges\")\n",
    "ax[3].set_title(\"Balanced Train Data 3\")\n",
    "\n",
    "print(\"\\nClass Percentages After Splitting\")\n",
    "print(\"Percent Neg Train1: \", (df_train1['class'].to_list()).count(0)/len(df_train1), \"Percent Pos Train 1: \", (df_train1['class'].to_list()).count(1)/len(df_train1))\n",
    "print(\"Percent Neg Train2: \", (df_train2['class'].to_list()).count(0)/len(df_train2), \"Percent Pos Train 2: \", (df_train2['class'].to_list()).count(1)/len(df_train2))\n",
    "print(\"Percent Neg Train3: \", (df_train3['class'].to_list()).count(0)/len(df_train3), \"Percent Pos Train 3: \", (df_train3['class'].to_list()).count(1)/len(df_train3))\n",
    "\n",
    "train_dataloaders = [DataLoader(CustomImageDataset(df_train1, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\n",
    "                     DataLoader(CustomImageDataset(df_train2, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\n",
    "                     DataLoader(CustomImageDataset(df_train3, transform=transform), batch_size=BATCH_SIZE, shuffle=True)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make positive class 10% the size of neg to make scew in pos class more noticable\n",
    "df_train_pos_10Percent = df_train_pos.sample(n=len(df_train_neg)*0.1, replace=False)\n",
    "num_folds=int(len(df_train_neg)/len(df_train_pos_10Percent))\n",
    "print(num_folds)\n",
    "\n",
    "samples_per_split = len(df_train_neg)/num_folds\n",
    "\n",
    "folds = [df_train_neg[i*samples_per_split:(i+1)*samples_per_split] for i in range(num_folds)]\n",
    "df_train_folds = [pd.concat([folds[i], df_train_pos_10Percent], ignore_index=True) for i in range(num_folds)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate performance measures\n",
    "def compute_performance(yhat, y, pos_cutoff, evaluation_phase='validation',\n",
    "                        model_id=0):\n",
    "\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat >= pos_cutoff, y == 1).numpy())\n",
    "    tn = sum(np.logical_and(yhat < pos_cutoff, y == 0).numpy())\n",
    "    fp = sum(np.logical_and(yhat >= pos_cutoff, y == 0).numpy())\n",
    "    fn = sum(np.logical_and(yhat < pos_cutoff, y == 1).numpy())\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "\n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "\n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "\n",
    "    balanced_acc = 0.5*(sensitivity+specificity)\n",
    "    #fMeasure =  2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    auroc = roc_auc_score(y, yhat)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Balanced Accuracy: \", balanced_acc,\" Specificity: \",specificity, \" AUROC Score: \", auroc,\n",
    "          \" Sensitivity: \", sensitivity,\" Precision: \", precision)\n",
    "    # Log results to WandB\n",
    "    wandb.log({\"(model-{}-{}) Balanced Accuracy\".format(model_id, evaluation_phase): balanced_acc,\n",
    "               \"(model-{}-{}) Specificity\".format(model_id, evaluation_phase): specificity,\n",
    "               \"(model-{}-{}) Sensitivity\".format(model_id, evaluation_phase): sensitivity,\n",
    "               \"(model-{}-{}) Precision\".format(model_id, evaluation_phase): precision,\n",
    "               \"(model-{}-{}) AUROC Score\".format(model_id, evaluation_phase): auroc},\n",
    "              commit=False)\n",
    "\n",
    "\n",
    "def train(model, dataloader_train, dataloader_val, device='cpu', epochs=10, early_stop=2, lr=0.001,\n",
    "          loss_function='bce_with_logits_loss', model_id=0, verbose=True):\n",
    "\n",
    "    opt = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    lowest_val_loss, train_loss = np.inf, 0\n",
    "    lowest_val_epoch = 0\n",
    "    epochs_wo_improvement = 0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    train_losses, val_losses=[], []\n",
    "    train_preds, train_targets_list = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in tqdm(dataloader_train):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_output = model.forward(inputs).squeeze()\n",
    "            train_preds+=train_output\n",
    "            train_targets_list+=targets\n",
    "            loss = criterion(train_output, targets.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_train_loss+=loss\n",
    "\n",
    "        compute_performance(torch.sigmoid(torch.Tensor(train_preds)), torch.Tensor(train_targets_list), 0.5, evaluation_phase='training', model_id=model_id)\n",
    "        epoch_train_loss = epoch_train_loss.item()/((len(dataloader_train.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        #VALIDATION\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        val_preds, val_targets_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in tqdm(dataloader_val):\n",
    "\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "                val_output = model.forward(val_inputs).squeeze()\n",
    "                val_preds+=val_output\n",
    "                val_targets_list+=val_targets\n",
    "\n",
    "                epoch_val_loss += criterion(val_output, val_targets.float())\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss.item()/((len(dataloader_val.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            compute_performance(torch.sigmoid(torch.Tensor(val_preds)), torch.Tensor(val_targets_list), 0.5,\n",
    "                                evaluation_phase='validation', model_id=model_id)\n",
    "\n",
    "        if epoch_val_loss <= lowest_val_loss:\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            lowest_val_loss = epoch_val_loss\n",
    "            train_loss=epoch_train_loss\n",
    "            lowest_val_epoch=e\n",
    "            epochs_wo_improvement=0\n",
    "        else:\n",
    "            epochs_wo_improvement+=1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs), \"Loss: {:.4f}...\".format(epoch_train_loss), \"Val Loss: {:.4f}\".format(epoch_val_loss),)\n",
    "        # Log to wandb project\n",
    "        wandb.log({\"(model-{}) training_loss\".format(model_id): epoch_train_loss,\n",
    "                   \"(model-{}) validation_loss\".format(model_id): epoch_val_loss})\n",
    "\n",
    "        #early stopping\n",
    "        if epochs_wo_improvement>=early_stop:\n",
    "            if verbose:\n",
    "                print(\"Early Stop no improvement in validation loss in \"+str(early_stop)+\" validation steps\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLowest Validation Loss: \"+str(lowest_val_loss)+\" at epoch \"+str(lowest_val_epoch)+'\\n')\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    # Record model to wandb\n",
    "    wandb.watch(model)\n",
    "\n",
    "    run_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    torch.save({'model_state_dict': best_model}, './BestModels/'+str(run_ID)+'_E_'+str(lowest_val_epoch)+'_TL_'+str(round(train_loss, 4))+'_VL_'+str(round(lowest_val_loss, 4))+'.pt')\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_models(dataloaders, load_save=False):\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    if load_save:\n",
    "        ensemble = [models.efficientnet_b0(pretrained=True) for _ in dataloaders]\n",
    "\n",
    "        for model in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    else:\n",
    "        ensemble = [[models.efficientnet_b0(pretrained=True), data] for data in dataloaders]\n",
    "\n",
    "        for model, _ in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trained_ensemble(dataloader_train, dataloader_val, loss_function='bce_with_logits_loss'):\n",
    "    ensemble = init_models(dataloaders=dataloader_train)\n",
    "    trained_ensemble = []\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    model_id = 1\n",
    "    for mod, dataloaders_train in ensemble:\n",
    "        model, train_losses, val_losses = train(mod, dataloaders_train, dataloader_val, early_stop=1,\n",
    "                                                device=device, loss_function=loss_function, model_id=model_id)\n",
    "        trained_ensemble.append(model)\n",
    "        model_id += 1\n",
    "    return trained_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dataloader_test, device='cpu', loss_function='bce_with_logits_loss'):\n",
    "    for mod in models:\n",
    "        mod.to(device)\n",
    "        mod.eval()\n",
    "        mod.zero_grad(set_to_none=True)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_preds, test_targets_list = [], []\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_targets in tqdm(dataloader_test):\n",
    "\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "            batch_output=[]\n",
    "\n",
    "            for model in models:\n",
    "                batch_output.append(torch.sigmoid(model.forward(test_inputs).squeeze()).cpu().numpy())\n",
    "\n",
    "            #average models output\n",
    "            batch_output = np.column_stack(batch_output)\n",
    "            test_output = np.mean(batch_output, axis=1)\n",
    "\n",
    "            test_preds = np.hstack((test_preds, test_output))\n",
    "            test_targets_list+=test_targets\n",
    "            test_loss += criterion(torch.tensor(test_output).to(device), test_targets.float())\n",
    "\n",
    "        print(\"Test Loss: \", test_loss.item()/((len(dataloader_test.dataset)%BATCH_SIZE)*BATCH_SIZE))\n",
    "\n",
    "        compute_performance(torch.Tensor(test_preds), torch.Tensor(test_targets_list), 0.5, \n",
    "                            evaluation_phase=\"Ensemble Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestModel():\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 10,\n",
    "        \"loss_function\": \"bce_with_logits_loss\"\n",
    "    }\n",
    "    wandb.init(project=\"breast-histopathology-classification\",\n",
    "               entity=\"ece9603_project\",\n",
    "               job_type=\"model_training\",\n",
    "               config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    trained_ensemble = get_trained_ensemble(train_dataloader, val_dataloader, loss_function=config.loss_function)\n",
    "    ensemble_predict(trained_ensemble, test_dataloader, device='cpu', loss_function=config.loss_function)\n",
    "\n",
    "    # Done this training run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't really need this\n",
    "\n",
    "``` python\n",
    "load_save = False\n",
    "\n",
    "if load_save:\n",
    "    checkpoint1 = torch.load('BestModels/Ensemble_1/2021-11-21_17-33_E_3_TL_0.0162_VL_0.01.pt')\n",
    "    checkpoint2 = torch.load('BestModels/Ensemble_1/2021-11-21_17-41_E_0_TL_0.0187_VL_0.01.pt')\n",
    "    checkpoint3 = torch.load('BestModels/Ensemble_1/2021-11-21_18-03_E_3_TL_0.0161_VL_0.0097.pt')\n",
    "\n",
    "    trained_ensemble = init_models(dataloaders=train_dataloaders, load_save=load_save)\n",
    "\n",
    "    trained_ensemble[0].load_state_dict(checkpoint1['model_state_dict'])\n",
    "    trained_ensemble[1].load_state_dict(checkpoint2['model_state_dict'])\n",
    "    trained_ensemble[2].load_state_dict(checkpoint3['model_state_dict'])\n",
    "else:\n",
    "    trained_ensemble = get_trained_ensemble(val_dataloader)\n",
    "\n",
    "ensemble_predict(trained_ensemble, test_dataloader, device='cuda')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep\n",
    "wandb.agent(sweep_id, trainAndTestModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}