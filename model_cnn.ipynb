{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install wandb if it hasn't been installed on the system yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this once\n",
    "#! pip install wandb\n",
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "from ece9603_project import lossFunctions\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a small example, we can adjust this for future hyper parameter tuning\n",
    "wandb_config = wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"loss_function\": \"focal_tversky_loss\"\n",
    "}\n",
    "\n",
    "wandb.init(project=\"breast-histopathology-classification\",\n",
    "           entity=\"ece9603_project\",\n",
    "           job_type=\"model_training\",\n",
    "           config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.info = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.info.imgPath.values[idx]\n",
    "        label = self.info['class'].values[idx]\n",
    "        image = read_image(path, mode=torchvision.io.image.ImageReadMode.RGB).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"breastCancerDataframe.csv\", index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "patientIDs = df.patient.unique()\n",
    "print(\"Number of Unique Patients: \", len(patientIDs))\n",
    "\n",
    "patients_train, temp = train_test_split(patientIDs, test_size=0.3, random_state=42)\n",
    "patients_val, patients_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train = df.loc[df['patient'].isin(patients_train)]\n",
    "print(df_train.head())\n",
    "print(\"Number of Train Patients: \", df_train.patient.nunique())\n",
    "\n",
    "df_val = df.loc[df['patient'].isin(patients_val)]\n",
    "print(df_val.head())\n",
    "print(\"Number of Validation Patients: \", df_val.patient.nunique())\n",
    "\n",
    "df_test = df.loc[df['patient'].isin(patients_test)]\n",
    "print(df_test.head())\n",
    "print(\"Number of Test Patients: \", df_test.patient.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomRotation(45),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomImageDataset(df_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(df_val, transform=transform)\n",
    "test_dataset = CustomImageDataset(df_test, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model=models.resnet18(pretrained=True)\n",
    "print(model)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(256, 1))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(1280, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(256, 1))\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "#summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate performance measures\n",
    "def compute_performance(yhat, y, pos_cutoff):\n",
    "\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat >= pos_cutoff, y == 1).numpy())\n",
    "    tn = sum(np.logical_and(yhat < pos_cutoff, y == 0).numpy())\n",
    "    fp = sum(np.logical_and(yhat >= pos_cutoff, y == 0).numpy())\n",
    "    fn = sum(np.logical_and(yhat < pos_cutoff, y == 1).numpy())\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "\n",
    "    # Accuracy\n",
    "    #acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "\n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "\n",
    "    balanced_acc = 0.5*(sensitivity+specificity)\n",
    "    #fMeasure =  2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    # Print results\n",
    "    print(\"Balanced Accuracy: \", balanced_acc,\" Specificity: \",specificity,\n",
    "          \" Sensitivity: \",sensitivity,\" Precision: \",precision,)\n",
    "    # Log results to WandB\n",
    "    wandb.log({\"Balanced Accuracy\": balanced_acc,\n",
    "               \"Specificity\": specificity,\n",
    "               \"Sensitivity\": sensitivity,\n",
    "               \"Precision\": precision},\n",
    "              commit=False)\n",
    "\n",
    "#fMeasure =  2*((performance[1]*performance[2])/(performance[1]+performance[2]))\n",
    "\n",
    "def train(model, dataloader_train, dataloader_val, device='cpu', epochs=10, early_stop=2, lr=0.001, \n",
    "          loss_function='bce_with_logits_loss', verbose=True):\n",
    "\n",
    "    opt = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    #opt = torch.optim.Adam(model.fc.parameters(), lr=lr)\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    lowest_val_loss, train_loss = np.inf, 0\n",
    "    lowest_val_epoch = 0\n",
    "    epochs_wo_improvement = 0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    train_losses, val_losses=[], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in tqdm(dataloader_train):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            output = model.forward(inputs)\n",
    "            loss = criterion(output.squeeze(), targets.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_train_loss+=loss\n",
    "\n",
    "        epoch_train_loss = epoch_train_loss.item()/((len(dataloader_train.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        #VALIDATION\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_targets_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in tqdm(dataloader_val):\n",
    "\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "                model.zero_grad(set_to_none=True)\n",
    "\n",
    "                val_output = model.forward(val_inputs).squeeze()\n",
    "                val_preds+=val_output\n",
    "                val_targets_list+=val_targets\n",
    "\n",
    "                epoch_val_loss += criterion(val_output, val_targets.float())\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss.item()/((len(dataloader_val.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            compute_performance(torch.sigmoid(torch.Tensor(val_preds)), torch.Tensor(val_targets_list), 0.5)\n",
    "\n",
    "        if epoch_val_loss <= lowest_val_loss:\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            lowest_val_loss = epoch_val_loss\n",
    "            train_loss=epoch_train_loss\n",
    "            lowest_val_epoch=e\n",
    "            epochs_wo_improvement=0\n",
    "        else:\n",
    "            epochs_wo_improvement+=1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs), \"Loss: {:.4f}...\".format(epoch_train_loss), \"Val Loss: {:.4f}\".format(epoch_val_loss),)\n",
    "        # Log to wandb project\n",
    "        wandb.log({\"training_loss\": epoch_train_loss,\n",
    "                   \"validation_loss\": epoch_val_loss})\n",
    "            \n",
    "        #early stopping\n",
    "        if epochs_wo_improvement>=early_stop:\n",
    "            if verbose:\n",
    "                print(\"Early Stop no improvement in validation loss in \"+str(early_stop)+\" validation steps\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLowest Validation Loss: \"+str(lowest_val_loss)+\" at epoch \"+str(lowest_val_epoch)+'\\n')\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    # Record model to wandb\n",
    "    wandb.watch(model)\n",
    "\n",
    "    run_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    torch.save({'model_state_dict': best_model}, './BestModels/'+str(run_ID)+'_E_'+str(lowest_val_epoch)+'_TL_'+str(round(train_loss, 4))+'_VL_'+str(round(lowest_val_loss, 4))+'.pt')\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "model, train_losses, val_losses = train(model, train_dataloader, val_dataloader, device=device, loss_function=wandb_config['loss_function'])\n",
    "# Done this training run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "test_preds, test_targets_list = [], []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for test_inputs, test_targets in tqdm(test_dataloader):\n",
    "\n",
    "        test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        test_output = model.forward(test_inputs).squeeze()\n",
    "        test_preds+=test_output\n",
    "        test_targets_list+=test_targets\n",
    "\n",
    "        test_loss += criterion(test_output, test_targets.float())\n",
    "\n",
    "    print(\"Test Loss: \", test_loss.item()/((len(test_dataloader.dataset)%BATCH_SIZE)*BATCH_SIZE))\n",
    "\n",
    "    compute_performance(torch.sigmoid(torch.Tensor(test_preds)), torch.Tensor(test_targets_list), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percent Neg: \", (df_train['class'].to_list()).count(0)/len(df_train))\n",
    "print(\"Percent Pos: \", (df_train['class'].to_list()).count(1)/len(df_train))\n",
    "\n",
    "df_train_neg = df_train.loc[df_train['class']==0]\n",
    "df_train_pos = df_train.loc[df_train['class']==1]\n",
    "print(\"Neg Count: \", len(df_train_neg), \"Pos Count: \",len(df_train_pos))\n",
    "\n",
    "#split it into 3 since i would rater the model have an over represented positive class so the model\n",
    "#predicts pos more than neg since identifying pos is more important\n",
    "df_train1, temp = train_test_split(df_train_neg, train_size=1/3, random_state=42)\n",
    "df_train2, df_train3 = train_test_split(temp, train_size=1/2, random_state=42)\n",
    "df_train1 = pd.concat([df_train1, df_train_pos], ignore_index=True)\n",
    "df_train2 = pd.concat([df_train2, df_train_pos], ignore_index=True)\n",
    "df_train3 = pd.concat([df_train3, df_train_pos], ignore_index=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(25,5))\n",
    "sns.countplot(df_train['class'], ax=ax[0], palette=\"Reds\")\n",
    "ax[0].set_title(\"Original Train Data\")\n",
    "sns.countplot(df_train1['class'], ax=ax[1], palette=\"Blues\")\n",
    "ax[1].set_title(\"Balanced Train Data 1\")\n",
    "sns.countplot(df_train2['class'], ax=ax[2], palette=\"Greens\")\n",
    "ax[2].set_title(\"Balanced Train Data 2\")\n",
    "sns.countplot(df_train3['class'], ax=ax[3], palette=\"Oranges\")\n",
    "ax[3].set_title(\"Balanced Train Data 3\")\n",
    "\n",
    "print(\"\\nClass Percentages After Splitting\")\n",
    "print(\"Percent Neg Train1: \", (df_train1['class'].to_list()).count(0)/len(df_train1), \"Percent Pos Train 1: \", (df_train1['class'].to_list()).count(1)/len(df_train1))\n",
    "print(\"Percent Neg Train2: \", (df_train2['class'].to_list()).count(0)/len(df_train2), \"Percent Pos Train 2: \", (df_train2['class'].to_list()).count(1)/len(df_train2))\n",
    "print(\"Percent Neg Train3: \", (df_train3['class'].to_list()).count(0)/len(df_train3), \"Percent Pos Train 3: \", (df_train3['class'].to_list()).count(1)/len(df_train3))\n",
    "\n",
    "train_dataloaders = [DataLoader(CustomImageDataset(df_train1, transform=transform), batch_size=BATCH_SIZE, shuffle=True), DataLoader(CustomImageDataset(df_train2, transform=transform), batch_size=BATCH_SIZE, shuffle=True), DataLoader(CustomImageDataset(df_train3, transform=transform), batch_size=BATCH_SIZE, shuffle=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_models(dataloaders, load_save=False):\n",
    "    if load_save:\n",
    "        ensemble = [models.efficientnet_b0(pretrained=True) for _ in dataloaders]\n",
    "\n",
    "        for model in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    else:\n",
    "        ensemble = [[models.efficientnet_b0(pretrained=True), data] for data in dataloaders]\n",
    "\n",
    "        for model, _ in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = init_models(dataloaders=train_dataloaders)\n",
    "trained_ensemble = []\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "for mod, dataloader_train in ensemble:\n",
    "    model, train_losses, val_losses = train(mod, dataloader_train, val_dataloader, early_stop=1, device=device)\n",
    "    trained_ensemble.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dataloader_test, device='cpu'):\n",
    "    for mod in models:\n",
    "        mod.to(device)\n",
    "        mod.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    test_preds, test_targets_list = [], []\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_targets in tqdm(dataloader_test):\n",
    "\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "            batch_output=[]\n",
    "\n",
    "            for model in models:\n",
    "                model.zero_grad(set_to_none=True)\n",
    "                batch_output.append(torch.sigmoid(model.forward(test_inputs).squeeze()).cpu().numpy())\n",
    "\n",
    "            #average models output\n",
    "            batch_output = np.column_stack(batch_output)\n",
    "            test_output = np.mean(batch_output, axis=1)\n",
    "\n",
    "            test_preds = np.hstack((test_preds, test_output))\n",
    "            test_targets_list+=test_targets\n",
    "            test_loss += criterion(torch.tensor(test_output).to(device), test_targets.float())\n",
    "\n",
    "        print(\"Test Loss: \", test_loss.item()/((len(dataloader_test.dataset)%BATCH_SIZE)*BATCH_SIZE))\n",
    "\n",
    "        compute_performance(torch.Tensor(test_preds), torch.Tensor(test_targets_list), 0.5)\n",
    "\n",
    "load_save = True\n",
    "\n",
    "if load_save:\n",
    "    checkpoint1 = torch.load('BestModels/2021-11-21_17-33_E_3_TL_0.0162_VL_0.01.pt')\n",
    "    checkpoint2 = torch.load('BestModels/2021-11-21_17-41_E_0_TL_0.0187_VL_0.01.pt')\n",
    "    checkpoint3 = torch.load('BestModels/2021-11-21_18-03_E_3_TL_0.0161_VL_0.0097.pt')\n",
    "\n",
    "    trained_ensemble = init_models(dataloaders=train_dataloaders, load_save=load_save)\n",
    "\n",
    "    trained_ensemble[0].load_state_dict(checkpoint1['model_state_dict'])\n",
    "    trained_ensemble[1].load_state_dict(checkpoint2['model_state_dict'])\n",
    "    trained_ensemble[2].load_state_dict(checkpoint3['model_state_dict'])\n",
    "\n",
    "ensemble_predict(trained_ensemble, test_dataloader, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
