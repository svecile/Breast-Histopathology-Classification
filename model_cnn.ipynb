{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import wandb\n",
    "from ece9603_project import lossFunctions\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run once to setup connection to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# Setup sweep hyperparameters\\nsweep_config = {\\n    \\'name\\': \\'ensemble_sweep\\',\\n    \\'method\\': \\'grid\\',\\n    \\'parameters\\': {\\n        \\'learning_rate\\': {\\n            \\'values\\': [0.001]\\n        },\\n        \\'epochs\\': {\\n            \\'values\\': [10]\\n        },\\n        \\'loss_function\\': {\\n            \\'values\\': [\\'dice_loss\\',\\n                       \\'bce_dice_loss\\',\\n                       \\'jaccard_iou_loss\\',\\n                       \\'focal_loss\\',\\n                       \\'tversky_loss\\',\\n                       \\'focal_tversky_loss\\',\\n                       \\'bce_with_logits_loss\\']\\n        }\\n    }\\n}\\nsweep_id = wandb.sweep(sweep_config,\\n                       project=\"breast-histopathology-classification\",\\n                       entity=\"ece9603_project\")\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Setup sweep hyperparameters\n",
    "sweep_config = {\n",
    "    'name': 'ensemble_sweep',\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'loss_function': {\n",
    "            'values': ['dice_loss',\n",
    "                       'bce_dice_loss',\n",
    "                       'jaccard_iou_loss',\n",
    "                       'focal_loss',\n",
    "                       'tversky_loss',\n",
    "                       'focal_tversky_loss',\n",
    "                       'bce_with_logits_loss']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       project=\"breast-histopathology-classification\",\n",
    "                       entity=\"ece9603_project\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.info = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.info.imgPath.values[idx]\n",
    "        label = self.info['class'].values[idx]\n",
    "        image = read_image(path, mode=torchvision.io.image.ImageReadMode.RGB).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient  class  posX  posY                                         imgPath\n",
      "0    12954      0  1151  1401  data/12954/0/12954_idx5_x1151_y1401_class0.png\n",
      "1    12954      0  1951  2901  data/12954/0/12954_idx5_x1951_y2901_class0.png\n",
      "2    12954      0   151   501    data/12954/0/12954_idx5_x151_y501_class0.png\n",
      "3    12954      0  1701  2251  data/12954/0/12954_idx5_x1701_y2251_class0.png\n",
      "4    12954      0  1501  2001  data/12954/0/12954_idx5_x1501_y2001_class0.png\n",
      "Number of Unique Patients:  279\n",
      "   patient  class  posX  posY                                         imgPath\n",
      "0    12954      0  1151  1401  data/12954/0/12954_idx5_x1151_y1401_class0.png\n",
      "1    12954      0  1951  2901  data/12954/0/12954_idx5_x1951_y2901_class0.png\n",
      "2    12954      0   151   501    data/12954/0/12954_idx5_x151_y501_class0.png\n",
      "3    12954      0  1701  2251  data/12954/0/12954_idx5_x1701_y2251_class0.png\n",
      "4    12954      0  1501  2001  data/12954/0/12954_idx5_x1501_y2001_class0.png\n",
      "Number of Train Patients:  195\n",
      "      patient  class  posX  posY  \\\n",
      "6566    15634      0  2401  1801   \n",
      "6567    15634      0  1001  1651   \n",
      "6568    15634      0  2051  1351   \n",
      "6569    15634      0  1301  2201   \n",
      "6570    15634      0  1351  1301   \n",
      "\n",
      "                                             imgPath  \n",
      "6566  data/15634/0/15634_idx5_x2401_y1801_class0.png  \n",
      "6567  data/15634/0/15634_idx5_x1001_y1651_class0.png  \n",
      "6568  data/15634/0/15634_idx5_x2051_y1351_class0.png  \n",
      "6569  data/15634/0/15634_idx5_x1301_y2201_class0.png  \n",
      "6570  data/15634/0/15634_idx5_x1351_y1301_class0.png  \n",
      "Number of Validation Patients:  42\n",
      "      patient  class  posX  posY  \\\n",
      "7926    13687      0  2251   251   \n",
      "7927    13687      0  2801   751   \n",
      "7928    13687      0  1551   701   \n",
      "7929    13687      0  1251   651   \n",
      "7930    13687      0  2851  1251   \n",
      "\n",
      "                                             imgPath  \n",
      "7926   data/13687/0/13687_idx5_x2251_y251_class0.png  \n",
      "7927   data/13687/0/13687_idx5_x2801_y751_class0.png  \n",
      "7928   data/13687/0/13687_idx5_x1551_y701_class0.png  \n",
      "7929   data/13687/0/13687_idx5_x1251_y651_class0.png  \n",
      "7930  data/13687/0/13687_idx5_x2851_y1251_class0.png  \n",
      "Number of Test Patients:  42\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(\"breastCancerDataframe.csv\", index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "patientIDs = df.patient.unique()\n",
    "print(\"Number of Unique Patients: \", len(patientIDs))\n",
    "\n",
    "patients_train, temp = train_test_split(patientIDs, test_size=0.3, random_state=42)\n",
    "patients_val, patients_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train = df.loc[df['patient'].isin(patients_train)]\n",
    "print(df_train.head())\n",
    "print(\"Number of Train Patients: \", df_train.patient.nunique())\n",
    "\n",
    "df_val = df.loc[df['patient'].isin(patients_val)]\n",
    "print(df_val.head())\n",
    "print(\"Number of Validation Patients: \", df_val.patient.nunique())\n",
    "\n",
    "df_test = df.loc[df['patient'].isin(patients_test)]\n",
    "print(df_test.head())\n",
    "print(\"Number of Test Patients: \", df_test.patient.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomImageDataset(df_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(df_val, transform=transform)\n",
    "test_dataset = CustomImageDataset(df_test, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprint(\"Percent Neg: \", (df_train[\\'class\\'].to_list()).count(0)/len(df_train))\\nprint(\"Percent Pos: \", (df_train[\\'class\\'].to_list()).count(1)/len(df_train))\\n\\ndf_train_neg = df_train.loc[df_train[\\'class\\']==0]\\ndf_train_pos = df_train.loc[df_train[\\'class\\']==1]\\nprint(\"Neg Count: \", len(df_train_neg), \"Pos Count: \",len(df_train_pos))\\n\\n#split it into 3 since i would rather the model have an over represented positive class so the model\\n#predicts pos more than neg since identifying pos is more important\\ndf_train1, temp = train_test_split(df_train_neg, train_size=1/3, random_state=42)\\ndf_train2, df_train3 = train_test_split(temp, train_size=1/2, random_state=42)\\ndf_train1 = pd.concat([df_train1, df_train_pos], ignore_index=True)\\ndf_train2 = pd.concat([df_train2, df_train_pos], ignore_index=True)\\ndf_train3 = pd.concat([df_train3, df_train_pos], ignore_index=True)\\n\\n\\nfig, ax = plt.subplots(1,4,figsize=(25,5))\\nsns.countplot(df_train[\\'class\\'], ax=ax[0], palette=\"Reds\")\\nax[0].set_title(\"Original Train Data\")\\nsns.countplot(df_train1[\\'class\\'], ax=ax[1], palette=\"Blues\")\\nax[1].set_title(\"Balanced Train Data 1\")\\nsns.countplot(df_train2[\\'class\\'], ax=ax[2], palette=\"Greens\")\\nax[2].set_title(\"Balanced Train Data 2\")\\nsns.countplot(df_train3[\\'class\\'], ax=ax[3], palette=\"Oranges\")\\nax[3].set_title(\"Balanced Train Data 3\")\\n\\nprint(\"\\nClass Percentages After Splitting\")\\nprint(\"Percent Neg Train1: \", (df_train1[\\'class\\'].to_list()).count(0)/len(df_train1), \"Percent Pos Train 1: \", (df_train1[\\'class\\'].to_list()).count(1)/len(df_train1))\\nprint(\"Percent Neg Train2: \", (df_train2[\\'class\\'].to_list()).count(0)/len(df_train2), \"Percent Pos Train 2: \", (df_train2[\\'class\\'].to_list()).count(1)/len(df_train2))\\nprint(\"Percent Neg Train3: \", (df_train3[\\'class\\'].to_list()).count(0)/len(df_train3), \"Percent Pos Train 3: \", (df_train3[\\'class\\'].to_list()).count(1)/len(df_train3))\\n\\ntrain_dataloaders = [DataLoader(CustomImageDataset(df_train1, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\\n                     DataLoader(CustomImageDataset(df_train2, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\\n                     DataLoader(CustomImageDataset(df_train3, transform=transform), batch_size=BATCH_SIZE, shuffle=True)]\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(\"Percent Neg: \", (df_train['class'].to_list()).count(0)/len(df_train))\n",
    "print(\"Percent Pos: \", (df_train['class'].to_list()).count(1)/len(df_train))\n",
    "\n",
    "df_train_neg = df_train.loc[df_train['class']==0]\n",
    "df_train_pos = df_train.loc[df_train['class']==1]\n",
    "print(\"Neg Count: \", len(df_train_neg), \"Pos Count: \",len(df_train_pos))\n",
    "\n",
    "#split it into 3 since i would rather the model have an over represented positive class so the model\n",
    "#predicts pos more than neg since identifying pos is more important\n",
    "df_train1, temp = train_test_split(df_train_neg, train_size=1/3, random_state=42)\n",
    "df_train2, df_train3 = train_test_split(temp, train_size=1/2, random_state=42)\n",
    "df_train1 = pd.concat([df_train1, df_train_pos], ignore_index=True)\n",
    "df_train2 = pd.concat([df_train2, df_train_pos], ignore_index=True)\n",
    "df_train3 = pd.concat([df_train3, df_train_pos], ignore_index=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(25,5))\n",
    "sns.countplot(df_train['class'], ax=ax[0], palette=\"Reds\")\n",
    "ax[0].set_title(\"Original Train Data\")\n",
    "sns.countplot(df_train1['class'], ax=ax[1], palette=\"Blues\")\n",
    "ax[1].set_title(\"Balanced Train Data 1\")\n",
    "sns.countplot(df_train2['class'], ax=ax[2], palette=\"Greens\")\n",
    "ax[2].set_title(\"Balanced Train Data 2\")\n",
    "sns.countplot(df_train3['class'], ax=ax[3], palette=\"Oranges\")\n",
    "ax[3].set_title(\"Balanced Train Data 3\")\n",
    "\n",
    "print(\"\\nClass Percentages After Splitting\")\n",
    "print(\"Percent Neg Train1: \", (df_train1['class'].to_list()).count(0)/len(df_train1), \"Percent Pos Train 1: \", (df_train1['class'].to_list()).count(1)/len(df_train1))\n",
    "print(\"Percent Neg Train2: \", (df_train2['class'].to_list()).count(0)/len(df_train2), \"Percent Pos Train 2: \", (df_train2['class'].to_list()).count(1)/len(df_train2))\n",
    "print(\"Percent Neg Train3: \", (df_train3['class'].to_list()).count(0)/len(df_train3), \"Percent Pos Train 3: \", (df_train3['class'].to_list()).count(1)/len(df_train3))\n",
    "\n",
    "train_dataloaders = [DataLoader(CustomImageDataset(df_train1, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\n",
    "                     DataLoader(CustomImageDataset(df_train2, transform=transform), batch_size=BATCH_SIZE, shuffle=True),\n",
    "                     DataLoader(CustomImageDataset(df_train3, transform=transform), batch_size=BATCH_SIZE, shuffle=True)]\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#split training data for ensomble models\n",
    "def ensemble_train_split(df_train):\n",
    "    #seperate positive and negative samples\n",
    "    df_train_neg = df_train.loc[df_train['class']==0]\n",
    "    df_train_pos = df_train.loc[df_train['class']==1]\n",
    "\n",
    "    #make positive class 10% the size of neg to make skew in pos class more noticeable\n",
    "    df_train_pos_10Percent = df_train_pos.sample(n=int(len(df_train_neg)*0.1), replace=False, random_state=42)\n",
    "    num_folds=int(len(df_train_neg)/len(df_train_pos_10Percent))\n",
    "    print(num_folds)\n",
    "\n",
    "    samples_per_split = int(len(df_train_neg)/num_folds)\n",
    "    #save train dfs incase we want to graph later\n",
    "    train_dataloaders, train_dfs = [], []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        neg_fold = df_train_neg[i*samples_per_split:(i+1)*samples_per_split]\n",
    "        train_dfs.append(pd.concat([neg_fold, df_train_pos_10Percent], ignore_index=True))\n",
    "        train_dataloaders.append(DataLoader(CustomImageDataset(train_dfs[i], transform=transform), batch_size=BATCH_SIZE, shuffle=True))\n",
    "        print(\"Percent Neg Train fold_\"+str(i)+\": \", ((train_dfs[i])['class'].to_list()).count(0)/len(train_dfs[i]), \"Percent Pos Train fold_\"+str(i)+\": \", ((train_dfs[i])['class'].to_list()).count(1)/len(train_dfs[i]))\n",
    "\n",
    "    return train_dataloaders\n",
    "\n",
    "# Calculate performance measures\n",
    "def compute_performance(yhat, y, pos_cutoff, evaluation_phase='validation',\n",
    "                        model_id=0):\n",
    "\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat >= pos_cutoff, y == 1).numpy())\n",
    "    tn = sum(np.logical_and(yhat < pos_cutoff, y == 0).numpy())\n",
    "    fp = sum(np.logical_and(yhat >= pos_cutoff, y == 0).numpy())\n",
    "    fn = sum(np.logical_and(yhat < pos_cutoff, y == 1).numpy())\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "\n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "\n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "\n",
    "    balanced_acc = 0.5*(sensitivity+specificity)\n",
    "    #fMeasure =  2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    auroc = roc_auc_score(y, yhat)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Balanced Accuracy: \", balanced_acc,\" Specificity: \",specificity, \" AUROC Score: \", auroc,\n",
    "          \" Sensitivity: \", sensitivity,\" Precision: \", precision)\n",
    "    # Log results to WandB\n",
    "    wandb.log({\"(model-{}-{}) Balanced Accuracy\".format(model_id, evaluation_phase): balanced_acc,\n",
    "               \"(model-{}-{}) Specificity\".format(model_id, evaluation_phase): specificity,\n",
    "               \"(model-{}-{}) Sensitivity\".format(model_id, evaluation_phase): sensitivity,\n",
    "               \"(model-{}-{}) Precision\".format(model_id, evaluation_phase): precision,\n",
    "               \"(model-{}-{}) AUROC Score\".format(model_id, evaluation_phase): auroc},\n",
    "              commit=False)\n",
    "\n",
    "def train(model, dataloader_train, dataloader_val, device='cpu', epochs=10, early_stop=2, lr=0.001,\n",
    "          loss_function='bce_with_logits_loss', model_id=0, verbose=True):\n",
    "\n",
    "    opt = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    lowest_val_loss, train_loss = np.inf, 0\n",
    "    lowest_val_epoch = 0\n",
    "    epochs_wo_improvement = 0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    train_losses, val_losses=[], []\n",
    "    train_preds, train_targets_list = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in tqdm(dataloader_train):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_output = model.forward(inputs).squeeze()\n",
    "            train_preds+=train_output\n",
    "            train_targets_list+=targets\n",
    "            loss = criterion(train_output, targets.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_train_loss+=loss\n",
    "\n",
    "        compute_performance(torch.sigmoid(torch.Tensor(train_preds)), torch.Tensor(train_targets_list), 0.5, evaluation_phase='training', model_id=model_id)\n",
    "        epoch_train_loss = epoch_train_loss.item()/((len(dataloader_train.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        #VALIDATION\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        val_preds, val_targets_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in tqdm(dataloader_val):\n",
    "\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "                val_output = model.forward(val_inputs).squeeze()\n",
    "                val_preds+=val_output\n",
    "                val_targets_list+=val_targets\n",
    "\n",
    "                epoch_val_loss += criterion(val_output, val_targets.float())\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss.item()/((len(dataloader_val.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            compute_performance(torch.sigmoid(torch.Tensor(val_preds)), torch.Tensor(val_targets_list), 0.5,\n",
    "                                evaluation_phase='validation', model_id=model_id)\n",
    "\n",
    "        if epoch_val_loss <= lowest_val_loss:\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            lowest_val_loss = epoch_val_loss\n",
    "            train_loss=epoch_train_loss\n",
    "            lowest_val_epoch=e\n",
    "            epochs_wo_improvement=0\n",
    "        else:\n",
    "            epochs_wo_improvement+=1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs), \"Loss: {:.4f}...\".format(epoch_train_loss), \"Val Loss: {:.4f}\".format(epoch_val_loss),)\n",
    "\n",
    "        # Log to wandb project\n",
    "        wandb.log({\"(model-{}) training_loss\".format(model_id): epoch_train_loss,\n",
    "                   \"(model-{}) validation_loss\".format(model_id): epoch_val_loss})\n",
    "\n",
    "        #early stopping\n",
    "        if epochs_wo_improvement>=early_stop:\n",
    "            if verbose:\n",
    "                print(\"Early Stop no improvement in validation loss in \"+str(early_stop)+\" validation steps\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLowest Validation Loss: \"+str(lowest_val_loss)+\" at epoch \"+str(lowest_val_epoch)+'\\n')\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    # Record model to wandb\n",
    "    wandb.watch(model)\n",
    "\n",
    "    run_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    torch.save({'model_state_dict': best_model}, './BestModels/'+str(run_ID)+'_E_'+str(lowest_val_epoch)+'_TL_'+str(round(train_loss, 4))+'_VL_'+str(round(lowest_val_loss, 4))+'.pt')\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def init_models(dataloaders, load_save=False):\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    if load_save:\n",
    "        ensemble = [models.efficientnet_b0(pretrained=True) for _ in dataloaders]\n",
    "\n",
    "        for model in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    else:\n",
    "        ensemble = [[models.efficientnet_b0(pretrained=True), data] for data in dataloaders]\n",
    "\n",
    "        for model, _ in ensemble:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(256, 1))\n",
    "\n",
    "            model.apply(init_weights)\n",
    "    return ensemble\n",
    "\n",
    "def get_trained_ensemble(dataloaders_train, dataloader_val, loss_function='bce_with_logits_loss'):\n",
    "    ensemble = init_models(dataloaders=dataloaders_train)\n",
    "    trained_ensemble = []\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    model_id = 1\n",
    "    for mod, dataloader_train in ensemble:\n",
    "        model, train_losses, val_losses = train(mod, dataloader_train, dataloader_val, early_stop=1,\n",
    "                                                device=device, loss_function=loss_function, model_id=model_id)\n",
    "        trained_ensemble.append(model)\n",
    "        model_id += 1\n",
    "    return trained_ensemble\n",
    "\n",
    "def ensemble_predict(models, dataloader_test, device='cpu', loss_function='bce_with_logits_loss'):\n",
    "    for mod in models:\n",
    "        mod.to(device)\n",
    "        mod.eval()\n",
    "        mod.zero_grad(set_to_none=True)\n",
    "\n",
    "    test_loss = 0\n",
    "    test_preds, test_targets_list = [], []\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_targets in tqdm(dataloader_test):\n",
    "\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "            batch_output=[]\n",
    "\n",
    "            for model in models:\n",
    "                batch_output.append(torch.sigmoid(model.forward(test_inputs).squeeze()).cpu().numpy())\n",
    "\n",
    "            #average models output\n",
    "            batch_output = np.column_stack(batch_output)\n",
    "            test_output = np.mean(batch_output, axis=1)\n",
    "\n",
    "            test_preds = np.hstack((test_preds, test_output))\n",
    "            test_targets_list+=test_targets\n",
    "            test_loss += criterion(torch.tensor(test_output).to(device), test_targets.float())\n",
    "\n",
    "        print(\"Test Loss: \", test_loss.item()/((len(dataloader_test.dataset)%BATCH_SIZE)*BATCH_SIZE))\n",
    "\n",
    "        compute_performance(torch.Tensor(test_preds), torch.Tensor(test_targets_list), 0.5, \n",
    "                            evaluation_phase=\"Ensemble Test\")\n",
    "\n",
    "\n",
    "def trainAndTestModel():\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 10,\n",
    "        \"loss_function\": \"bce_with_logits_loss\"\n",
    "    }\n",
    "    wandb.init(project=\"breast-histopathology-classification\",\n",
    "               entity=\"ece9603_project\",\n",
    "               job_type=\"model_training\",\n",
    "               config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    trained_ensemble = get_trained_ensemble(ensemble_train_split(df_train), val_dataloader, loss_function=config.loss_function)\n",
    "    ensemble_predict(trained_ensemble, test_dataloader, device='cuda', loss_function=config.loss_function)\n",
    "\n",
    "    # Done this training run\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't really need this\n",
    "\n",
    "``` python\n",
    "load_save = False\n",
    "\n",
    "if load_save:\n",
    "    checkpoint1 = torch.load('BestModels/Ensemble_1/2021-11-21_17-33_E_3_TL_0.0162_VL_0.01.pt')\n",
    "    checkpoint2 = torch.load('BestModels/Ensemble_1/2021-11-21_17-41_E_0_TL_0.0187_VL_0.01.pt')\n",
    "    checkpoint3 = torch.load('BestModels/Ensemble_1/2021-11-21_18-03_E_3_TL_0.0161_VL_0.0097.pt')\n",
    "\n",
    "    trained_ensemble = init_models(dataloaders=train_dataloaders, load_save=load_save)\n",
    "\n",
    "    trained_ensemble[0].load_state_dict(checkpoint1['model_state_dict'])\n",
    "    trained_ensemble[1].load_state_dict(checkpoint2['model_state_dict'])\n",
    "    trained_ensemble[2].load_state_dict(checkpoint3['model_state_dict'])\n",
    "else:\n",
    "    trained_ensemble = get_trained_ensemble(val_dataloader)\n",
    "\n",
    "ensemble_predict(trained_ensemble, test_dataloader, device='cuda')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:3cpcfyx1) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 7204... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c073e1f142144a4b9740779ad8f2fec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>(model-1) training_loss</td><td>▁</td></tr><tr><td>(model-1) validation_loss</td><td>▁</td></tr><tr><td>(model-1-training) AUROC Score</td><td>▁█</td></tr><tr><td>(model-1-training) Balanced Accuracy</td><td>▁█</td></tr><tr><td>(model-1-training) Precision</td><td>▁█</td></tr><tr><td>(model-1-training) Sensitivity</td><td>▁█</td></tr><tr><td>(model-1-training) Specificity</td><td>▁█</td></tr><tr><td>(model-1-validation) AUROC Score</td><td>▁</td></tr><tr><td>(model-1-validation) Balanced Accuracy</td><td>▁</td></tr><tr><td>(model-1-validation) Precision</td><td>▁</td></tr><tr><td>(model-1-validation) Sensitivity</td><td>▁</td></tr><tr><td>(model-1-validation) Specificity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>(model-1) training_loss</td><td>0.00503</td></tr><tr><td>(model-1) validation_loss</td><td>0.01322</td></tr><tr><td>(model-1-training) AUROC Score</td><td>0.93736</td></tr><tr><td>(model-1-training) Balanced Accuracy</td><td>0.86436</td></tr><tr><td>(model-1-training) Precision</td><td>0.86034</td></tr><tr><td>(model-1-training) Sensitivity</td><td>0.86995</td></tr><tr><td>(model-1-training) Specificity</td><td>0.85877</td></tr><tr><td>(model-1-validation) AUROC Score</td><td>0.9118</td></tr><tr><td>(model-1-validation) Balanced Accuracy</td><td>0.83305</td></tr><tr><td>(model-1-validation) Precision</td><td>0.7061</td></tr><tr><td>(model-1-validation) Sensitivity</td><td>0.83867</td></tr><tr><td>(model-1-validation) Specificity</td><td>0.82743</td></tr></table>\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">dauntless-brook-45</strong>: <a href=\"https://wandb.ai/ece9603_project/breast-histopathology-classification/runs/3cpcfyx1\" target=\"_blank\">https://wandb.ai/ece9603_project/breast-histopathology-classification/runs/3cpcfyx1</a><br/>\nFind logs at: <code>.\\wandb\\run-20211125_205854-3cpcfyx1\\logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:3cpcfyx1). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/ece9603_project/breast-histopathology-classification/runs/2xffr30x\" target=\"_blank\">treasured-sky-46</a></strong> to <a href=\"https://wandb.ai/ece9603_project/breast-histopathology-classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Percent Neg Train fold_0:  0.5 Percent Pos Train fold_0:  0.5\n",
      "Percent Neg Train fold_1:  0.5 Percent Pos Train fold_1:  0.5\n",
      "Percent Neg Train fold_2:  0.5 Percent Pos Train fold_2:  0.5\n",
      "Percent Neg Train fold_3:  0.5 Percent Pos Train fold_3:  0.5\n",
      "Percent Neg Train fold_4:  0.5 Percent Pos Train fold_4:  0.5\n",
      "Percent Neg Train fold_5:  0.5 Percent Pos Train fold_5:  0.5\n",
      "Percent Neg Train fold_6:  0.5 Percent Pos Train fold_6:  0.5\n",
      "Percent Neg Train fold_7:  0.5 Percent Pos Train fold_7:  0.5\n",
      "Percent Neg Train fold_8:  0.5 Percent Pos Train fold_8:  0.5\n",
      "Percent Neg Train fold_9:  0.5 Percent Pos Train fold_9:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:50<00:00,  4.42it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 12098 tn: 12196 fp: 2072 fn: 2170\n",
      "Balanced Accuracy:  0.8513456686291001  Specificity:  0.8547799271096159  AUROC Score:  0.9267025587478404  Sensitivity:  0.8479114101485843  Precision:  0.8537755822159492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:12<00:00,  4.58it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 12259 tn: 23238 fp: 5336 fn: 1867\n",
      "Balanced Accuracy:  0.8405445863687927  Specificity:  0.8132568068873801  AUROC Score:  0.9153557745709725  Sensitivity:  0.8678323658502053  Precision:  0.6967320261437908\n",
      "Epoch: 0/10... Loss: 0.0050... Val Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:49<00:00,  4.52it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 24665 tn: 24681 fp: 3855 fn: 3871\n",
      "Balanced Accuracy:  0.8646271376506869  Specificity:  0.8649074852817493  AUROC Score:  0.9374611575930549  Sensitivity:  0.8643467900196243  Precision:  0.8648316970546984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:12<00:00,  4.61it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11704 tn: 24348 fp: 4226 fn: 2422\n",
      "Balanced Accuracy:  0.840323211347054  Specificity:  0.8521033107020368  AUROC Score:  0.9177088197344697  Sensitivity:  0.8285431119920713  Precision:  0.7347143753923415\n",
      "Epoch: 1/10... Loss: 0.0043... Val Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:49<00:00,  4.48it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 37259 tn: 37216 fp: 5588 fn: 5545\n",
      "Balanced Accuracy:  0.8699537426408747  Specificity:  0.869451453135221  AUROC Score:  0.9421930051423439  Sensitivity:  0.8704560321465283  Precision:  0.8695824678507247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:13<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 12494 tn: 23001 fp: 5573 fn: 1632\n",
      "Balanced Accuracy:  0.8447154547963825  Specificity:  0.8049625533701967  AUROC Score:  0.9223430941760337  Sensitivity:  0.8844683562225684  Precision:  0.6915370565118725\n",
      "Epoch: 2/10... Loss: 0.0041... Val Loss: 0.0138\n",
      "Early Stop no improvement in validation loss in 1 validation steps\n",
      "\n",
      "Lowest Validation Loss: 0.012669811907567476 at epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:49<00:00,  4.52it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11961 tn: 12117 fp: 2151 fn: 2307\n",
      "Balanced Accuracy:  0.843776282590412  Specificity:  0.8492430613961311  AUROC Score:  0.9210251434374643  Sensitivity:  0.838309503784693  Precision:  0.8475765306122449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:12<00:00,  4.60it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11673 tn: 24711 fp: 3863 fn: 2453\n",
      "Balanced Accuracy:  0.8455778722234126  Specificity:  0.864807167354938  AUROC Score:  0.9259587425040567  Sensitivity:  0.8263485770918872  Precision:  0.7513516992790937\n",
      "Epoch: 0/10... Loss: 0.0052... Val Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:50<00:00,  4.44it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 24287 tn: 24629 fp: 3907 fn: 4249\n",
      "Balanced Accuracy:  0.8570927950658818  Specificity:  0.863085225679843  AUROC Score:  0.9322075069925788  Sensitivity:  0.8511003644519204  Precision:  0.8614244165425268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:13<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11482 tn: 25043 fp: 3531 fn: 2644\n",
      "Balanced Accuracy:  0.8446267660489346  Specificity:  0.8764261216490515  AUROC Score:  0.924476380872005  Sensitivity:  0.8128274104488178  Precision:  0.7648038366748817\n",
      "Epoch: 1/10... Loss: 0.0044... Val Loss: 0.0118\n",
      "Early Stop no improvement in validation loss in 1 validation steps\n",
      "\n",
      "Lowest Validation Loss: 0.011673768099985625 at epoch 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:49<00:00,  4.48it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11955 tn: 12031 fp: 2237 fn: 2313\n",
      "Balanced Accuracy:  0.8405522848331932  Specificity:  0.843215587328287  AUROC Score:  0.9183642503640315  Sensitivity:  0.8378889823380993  Precision:  0.8423759864712514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [01:14<00:00,  4.50it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 11956 tn: 24123 fp: 4451 fn: 2170\n",
      "Balanced Accuracy:  0.8453057881876855  Specificity:  0.844229019388255  AUROC Score:  0.9228060468115847  Sensitivity:  0.846382556987116  Precision:  0.7287133540561955\n",
      "Epoch: 0/10... Loss: 0.0053... Val Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [00:50<00:00,  4.44it/s]\n",
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 24250 tn: 24399 fp: 4137 fn: 4286\n",
      "Balanced Accuracy:  0.852414493972526  Specificity:  0.8550252312867956  AUROC Score:  0.9302386318721225  Sensitivity:  0.8498037566582562  Precision:  0.8542642759009406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 204/334 [00:45<00:28,  4.55it/s]"
     ]
    }
   ],
   "source": [
    "# Run the sweep\n",
    "#wandb.agent(sweep_id, trainAndTestModel)\n",
    "\n",
    "trainAndTestModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}