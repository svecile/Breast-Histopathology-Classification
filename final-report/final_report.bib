@misc{spotify_dataset,
title = {Spotify {Dataset} 1921-2020, 160k+ {Tracks}},
author = {Eren Ay, Yamac},
url = {https://kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks},
abstract = {Audio features of 175k+ songs released in between 1921 and 2021 (UPDATED)},
language = {en},
urldate = {2021-03-22},
}

@article{context_aware_artist_popularity,
author={Y. {Matsumoto} and R. {Harakawa} and T. {Ogawa} and M. {Haseyama}},
journal={IEEE Access},
title={Context-Aware Network Analysis of Music Streaming Services for Popularity Estimation of Artists},
year={2020},
volume={8},
number={},
pages={48673-48685},
doi={10.1109/ACCESS.2020.2978281}}

@article{tsne_paper,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
year = {2008},
month = {11},
pages = {2579-2605},
title = {Viualizing data using t-SNE},
volume = {9},
journal = {Journal of Machine Learning Research}
}

@article{seaborn,
  doi = {10.21105/joss.03021},
  url = {https://doi.org/10.21105/joss.03021},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {60},
  pages = {3021},
  author = {Michael L. Waskom},
  title = {seaborn: statistical data visualization},
  journal = {Journal of Open Source Software}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{neuro-evolution,
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
title = {Evolving Neural Networks through Augmenting Topologies},
year = {2002},
issue_date = {Summer 2002},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {10},
number = {2},
issn = {1063-6560},
url = {https://doi.org/10.1162/106365602320169811},
doi = {10.1162/106365602320169811},
abstract = {An important question in neuroevolution is how to gain an advantage from
            evolving neural network topologies along with weights. We present a method,
            NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best
            fixed-topology method on a challenging benchmark reinforcement learning task. We
            claim that the increased efficiency is due to (1) employing a principled method of
            crossover of different topologies, (2) protecting structural innovation using
            speciation, and (3) incrementally growing from minimal structure. We test this
            claim through a series of ablation studies that demonstrate that each component is
            necessary to the system as a whole and to each other. What results is
            significantly faster learning. NEAT is also an important contribution to GAs
            because it shows how it is possible for evolution to both optimize and complexify
            solutions simultaneously, offering the possibility of evolving increasingly
            complex solutions over generations, and strengthening the analogy with biological
            evolution.},
journal = {Evol. Comput.},
month = jun,
pages = {99–127},
numpages = {29},
keywords = {speciation, competing conventions, neural networks, neuroevolution,
            network topologies, genetic algorithms}
}
