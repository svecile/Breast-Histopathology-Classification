{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import wandb\n",
    "from ece9603_project import lossFunctions\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run once to setup connection to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup sweep hyperparameters\n",
    "sweep_config = {\n",
    "    'name': 'loss_function_evaluation',\n",
    "    'method': 'grid',\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'loss_function': {\n",
    "            'values': ['dice_loss',\n",
    "                       'bce_dice_loss',\n",
    "                       'jaccard_iou_loss',\n",
    "                       'focal_loss',\n",
    "                       'tversky_loss',\n",
    "                       'focal_tversky_loss',\n",
    "                       'bce_with_logits_loss']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       project=\"breast-histopathology-classification\",\n",
    "                       entity=\"ece9603_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.info = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.info.imgPath.values[idx]\n",
    "        label = self.info['class'].values[idx]\n",
    "        image = read_image(path, mode=torchvision.io.image.ImageReadMode.RGB).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"breastCancerDataframe.csv\", index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "patientIDs = df.patient.unique()\n",
    "print(\"Number of Unique Patients: \", len(patientIDs))\n",
    "\n",
    "patients_train, temp = train_test_split(patientIDs, test_size=0.3, random_state=42)\n",
    "patients_val, patients_test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train = df.loc[df['patient'].isin(patients_train)]\n",
    "print(df_train.head())\n",
    "print(\"Number of Train Patients: \", df_train.patient.nunique())\n",
    "\n",
    "df_val = df.loc[df['patient'].isin(patients_val)]\n",
    "print(df_val.head())\n",
    "print(\"Number of Validation Patients: \", df_val.patient.nunique())\n",
    "\n",
    "df_test = df.loc[df['patient'].isin(patients_test)]\n",
    "print(df_test.head())\n",
    "print(\"Number of Test Patients: \", df_test.patient.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        #transforms.RandomRotation(45),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = CustomImageDataset(df_train, transform=transform)\n",
    "val_dataset = CustomImageDataset(df_val, transform=transform)\n",
    "test_dataset = CustomImageDataset(df_test, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate performance measures\n",
    "def compute_performance(yhat, y, pos_cutoff, evaluation_phase='validation'):\n",
    "\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat >= pos_cutoff, y == 1).numpy())\n",
    "    tn = sum(np.logical_and(yhat < pos_cutoff, y == 0).numpy())\n",
    "    fp = sum(np.logical_and(yhat >= pos_cutoff, y == 0).numpy())\n",
    "    fn = sum(np.logical_and(yhat < pos_cutoff, y == 1).numpy())\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "\n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "\n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "\n",
    "    balanced_acc = 0.5*(sensitivity+specificity)\n",
    "\n",
    "    auroc = roc_auc_score(y, yhat)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Balanced Accuracy: \", balanced_acc,\" Specificity: \",specificity, \" AUROC Score: \", auroc,\n",
    "          \" Sensitivity: \", sensitivity,\" Precision: \", precision)\n",
    "\n",
    "    # Log results to WandB\n",
    "    wandb.log({\"{} Balanced Accuracy\".format(evaluation_phase): balanced_acc,\n",
    "               \"{} Specificity\".format(evaluation_phase): specificity,\n",
    "               \"{} Sensitivity\".format(evaluation_phase): sensitivity,\n",
    "               \"{} Precision\".format(evaluation_phase): precision,\n",
    "               \"{} AUROC Score\".format(evaluation_phase): auroc},\n",
    "              commit=False)\n",
    "\n",
    "def init_model():\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(1280, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Linear(256, 1))\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(model, dataloader_train, dataloader_val, device='cpu', epochs=10, early_stop=2, lr=0.001,\n",
    "          loss_function='bce_with_logits_loss', verbose=True):\n",
    "\n",
    "    opt = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    lowest_val_loss, train_loss = np.inf, 0\n",
    "    lowest_val_epoch = 0\n",
    "    epochs_wo_improvement = 0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    train_losses, val_losses=[], []\n",
    "    train_preds, train_targets_list = [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in tqdm(dataloader_train):\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            train_output = model.forward(inputs).squeeze()\n",
    "            train_preds+=train_output\n",
    "            train_targets_list+=targets\n",
    "            loss = criterion(train_output, targets.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_train_loss+=loss\n",
    "\n",
    "        compute_performance(torch.sigmoid(torch.Tensor(train_preds)), torch.Tensor(train_targets_list), 0.5, evaluation_phase='training')\n",
    "        epoch_train_loss = epoch_train_loss.item()/((len(dataloader_train.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        #VALIDATION\n",
    "\n",
    "        model.eval()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        val_preds, val_targets_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in tqdm(dataloader_val):\n",
    "\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "                val_output = model.forward(val_inputs).squeeze()\n",
    "                val_preds+=val_output\n",
    "                val_targets_list+=val_targets\n",
    "\n",
    "                epoch_val_loss += criterion(val_output, val_targets.float())\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss.item()/((len(dataloader_val.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "            compute_performance(torch.sigmoid(torch.Tensor(val_preds)), torch.Tensor(val_targets_list), 0.5,\n",
    "                                evaluation_phase='validation')\n",
    "\n",
    "        if epoch_val_loss <= lowest_val_loss:\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            lowest_val_loss = epoch_val_loss\n",
    "            train_loss=epoch_train_loss\n",
    "            lowest_val_epoch=e\n",
    "            epochs_wo_improvement=0\n",
    "        else:\n",
    "            epochs_wo_improvement+=1\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs), \"Loss: {:.4f}...\".format(epoch_train_loss), \"Val Loss: {:.4f}\".format(epoch_val_loss),)\n",
    "\n",
    "        # Log to wandb project\n",
    "        wandb.log({\"training_loss\": epoch_train_loss, \"validation_loss\": epoch_val_loss})\n",
    "\n",
    "        #early stopping\n",
    "        if epochs_wo_improvement>=early_stop:\n",
    "            if verbose:\n",
    "                print(\"Early Stop no improvement in validation loss in \"+str(early_stop)+\" validation steps\")\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLowest Validation Loss: \"+str(lowest_val_loss)+\" at epoch \"+str(lowest_val_epoch)+'\\n')\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Record model to wandb\n",
    "    wandb.watch(model)\n",
    "\n",
    "    run_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    torch.save({'model_state_dict': best_model}, './BestModels/'+str(run_ID)+'_'+loss_function+'_E_'+str(lowest_val_epoch)+'_TL_'+str(round(train_loss, 4))+'_VL_'+str(round(lowest_val_loss, 4))+'.pt')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_dataloader, loss_function='bce_with_logits_loss', device='cuda'):\n",
    "    #TESTING PHASE\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    test_preds, test_targets_list = [], []\n",
    "    test_loss = 0\n",
    "    criterion = lossFunctions.getLossFunction(loss_function)\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_inputs, test_targets in tqdm(test_dataloader):\n",
    "\n",
    "            test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "\n",
    "            test_output = model.forward(test_inputs).squeeze()\n",
    "            test_preds+=test_output\n",
    "            test_targets_list+=test_targets\n",
    "\n",
    "            test_loss += criterion(test_output, test_targets.float())\n",
    "\n",
    "        test_loss = test_loss.item()/((len(test_dataloader.dataset)%BATCH_SIZE)*BATCH_SIZE)\n",
    "        print(\"Test Loss: \", test_loss)\n",
    "\n",
    "        compute_performance(torch.sigmoid(torch.Tensor(test_preds)), torch.Tensor(test_targets_list), 0.5, evaluation_phase='testing')\n",
    "        # Log to wandb project\n",
    "        wandb.log({\"test_loss\": test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTestModel():\n",
    "    config_defaults = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 10,\n",
    "        \"loss_function\": \"bce_with_logits_loss\"\n",
    "    }\n",
    "    wandb.init(project=\"breast-histopathology-classification\",\n",
    "               entity=\"ece9603_project\",\n",
    "               job_type=\"model_training\",\n",
    "               config=config_defaults)\n",
    "\n",
    "    config = wandb.config\n",
    "    single_model = init_model()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    trained_model = train(single_model, train_dataloader, val_dataloader, loss_function=config.loss_function, device='cuda')\n",
    "    test_model(trained_model, test_dataloader, loss_function=config.loss_function)\n",
    "\n",
    "    # Done this training run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep\n",
    "wandb.agent(sweep_id, trainAndTestModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
